<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>carvekit.ml.wrap.yolov4 API documentation</title>
<meta name="description" content="Source url: https://github.com/OPHoperHPO/image-background-remove-tool
Author: Nikita Selin (OPHoperHPO)[https://github.com/OPHoperHPO].
License: …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>carvekit.ml.wrap.yolov4</code></h1>
</header>
<section id="section-intro">
<p>Source url: <a href="https://github.com/OPHoperHPO/image-background-remove-tool">https://github.com/OPHoperHPO/image-background-remove-tool</a>
Author: Nikita Selin (OPHoperHPO)[<a href="https://github.com/OPHoperHPO].">https://github.com/OPHoperHPO].</a>
License: Apache License 2.0</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Source url: https://github.com/OPHoperHPO/image-background-remove-tool
Author: Nikita Selin (OPHoperHPO)[https://github.com/OPHoperHPO].
License: Apache License 2.0
&#34;&#34;&#34;

import pathlib

import PIL.Image
import PIL.Image
import numpy as np
import pydantic
import torch
from torch.autograd import Variable
from typing import List, Union

from carvekit.ml.arch.yolov4.models import Yolov4
from carvekit.ml.arch.yolov4.utils import post_processing
from carvekit.ml.files.models_loc import yolov4_coco_pretrained
from carvekit.utils.image_utils import load_image, convert_image
from carvekit.utils.models_utils import get_precision_autocast, cast_network
from carvekit.utils.pool_utils import thread_pool_processing, batch_generator

__all__ = [&#34;YoloV4_COCO&#34;, &#34;SimplifiedYoloV4&#34;]


class Object(pydantic.BaseModel):
    &#34;&#34;&#34;Object class&#34;&#34;&#34;

    class_name: str
    confidence: float
    x1: int
    y1: int
    x2: int
    y2: int


class YoloV4_COCO(Yolov4):
    &#34;&#34;&#34;YoloV4 COCO model wrapper&#34;&#34;&#34;

    def __init__(
        self,
        n_classes: int = 80,
        device=&#34;cpu&#34;,
        classes: List[str] = None,
        input_image_size: Union[List[int], int] = 608,
        batch_size: int = 4,
        load_pretrained: bool = True,
        fp16: bool = False,
        model_path: Union[str, pathlib.Path] = None,
    ):
        &#34;&#34;&#34;
        Initialize the YoloV4 COCO.

        Args:
            n_classes: number of classes
            device: processing device
            input_image_size: input image size
            batch_size: the number of images that the neural network processes in one run
            fp16: use fp16 precision
            model_path: path to model weights
            load_pretrained: load pretrained weights
        &#34;&#34;&#34;
        if model_path is None:
            model_path = yolov4_coco_pretrained()
        self.fp16 = fp16
        self.device = device
        self.batch_size = batch_size
        if isinstance(input_image_size, list):
            self.input_image_size = input_image_size[:2]
        else:
            self.input_image_size = (input_image_size, input_image_size)

        if load_pretrained:
            state_dict = torch.load(model_path, map_location=&#34;cpu&#34;)
            self.classes = state_dict[&#34;classes&#34;]
            super().__init__(n_classes=len(state_dict[&#34;classes&#34;]), inference=True)
            self.load_state_dict(state_dict[&#34;state&#34;])
        else:
            self.classes = classes
            super().__init__(n_classes=n_classes, inference=True)

        self.to(device)
        self.eval()

    def data_preprocessing(self, data: PIL.Image.Image) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;
        Transform input image to suitable data format for neural network

        Args:
            data: input image

        Returns:
            input for neural network

        &#34;&#34;&#34;
        image = data.resize(self.input_image_size)
        # noinspection PyTypeChecker
        image = np.array(image).astype(np.float32)
        image = image.transpose((2, 0, 1))
        image = image / 255.0
        image = torch.from_numpy(image).float()
        return torch.unsqueeze(image, 0).type(torch.FloatTensor)

    def data_postprocessing(
        self, data: List[torch.FloatTensor], images: List[PIL.Image.Image]
    ) -&gt; List[Object]:
        &#34;&#34;&#34;
        Transforms output data from neural network to suitable data
        format for using with other components of this framework.

        Args:
            data: output data from neural network
            images: input images


        Returns:
            list of objects for each image

        &#34;&#34;&#34;
        output = post_processing(0.4, 0.6, data)
        images_objects = []
        for image_idx, image_objects in enumerate(output):
            image_size = images[image_idx].size
            objects = []
            for obj in image_objects:
                objects.append(
                    Object(
                        class_name=self.classes[obj[6]],
                        confidence=obj[5],
                        x1=int(obj[0] * image_size[0]),
                        y1=int(obj[1] * image_size[1]),
                        x2=int(obj[2] * image_size[0]),
                        y2=int(obj[3] * image_size[1]),
                    )
                )
            images_objects.append(objects)

        return images_objects

    def __call__(
        self, images: List[Union[str, pathlib.Path, PIL.Image.Image]]
    ) -&gt; List[List[Object]]:
        &#34;&#34;&#34;
        Passes input images though neural network

        Args:
            images: input images

        Returns:
            list of objects for each image

        &#34;&#34;&#34;
        collect_masks = []
        autocast, dtype = get_precision_autocast(device=self.device, fp16=self.fp16)
        with autocast:
            cast_network(self, dtype)
            for image_batch in batch_generator(images, self.batch_size):
                converted_images = thread_pool_processing(
                    lambda x: convert_image(load_image(x)), image_batch
                )
                batches = torch.vstack(
                    thread_pool_processing(self.data_preprocessing, converted_images)
                )
                with torch.no_grad():
                    batches = Variable(batches).to(self.device)
                    out = super().__call__(batches)
                    out_cpu = [out_i.cpu() for out_i in out]
                    del batches, out
                out = self.data_postprocessing(out_cpu, converted_images)
                collect_masks += out

        return collect_masks


class SimplifiedYoloV4(YoloV4_COCO):
    &#34;&#34;&#34;
    The YoloV4 COCO classifier, but classifies only 7 supercategories.

    human - Scenes of people, such as portrait photographs
    animals - Scenes with animals
    objects - Scenes with normal objects
    cars - Scenes with cars
    other - Other scenes
    &#34;&#34;&#34;

    db = {
        &#34;human&#34;: [&#34;person&#34;],
        &#34;animals&#34;: [
            &#34;bird&#34;,
            &#34;cat&#34;,
            &#34;dog&#34;,
            &#34;horse&#34;,
            &#34;sheep&#34;,
            &#34;cow&#34;,
            &#34;elephant&#34;,
            &#34;bear&#34;,
            &#34;zebra&#34;,
            &#34;giraffe&#34;,
        ],
        &#34;cars&#34;: [
            &#34;car&#34;,
            &#34;motorbike&#34;,
            &#34;bus&#34;,
            &#34;truck&#34;,
        ],
        &#34;objects&#34;: [
            &#34;bicycle&#34;,
            &#34;traffic light&#34;,
            &#34;fire hydrant&#34;,
            &#34;stop sign&#34;,
            &#34;parking meter&#34;,
            &#34;bench&#34;,
            &#34;backpack&#34;,
            &#34;umbrella&#34;,
            &#34;handbag&#34;,
            &#34;tie&#34;,
            &#34;suitcase&#34;,
            &#34;frisbee&#34;,
            &#34;skis&#34;,
            &#34;snowboard&#34;,
            &#34;sports ball&#34;,
            &#34;kite&#34;,
            &#34;baseball bat&#34;,
            &#34;baseball glove&#34;,
            &#34;skateboard&#34;,
            &#34;surfboard&#34;,
            &#34;tennis racket&#34;,
            &#34;bottle&#34;,
            &#34;wine glass&#34;,
            &#34;cup&#34;,
            &#34;fork&#34;,
            &#34;knife&#34;,
            &#34;spoon&#34;,
            &#34;bowl&#34;,
            &#34;banana&#34;,
            &#34;apple&#34;,
            &#34;sandwich&#34;,
            &#34;orange&#34;,
            &#34;broccoli&#34;,
            &#34;carrot&#34;,
            &#34;hot dog&#34;,
            &#34;pizza&#34;,
            &#34;donut&#34;,
            &#34;cake&#34;,
            &#34;chair&#34;,
            &#34;sofa&#34;,
            &#34;pottedplant&#34;,
            &#34;bed&#34;,
            &#34;diningtable&#34;,
            &#34;toilet&#34;,
            &#34;tvmonitor&#34;,
            &#34;laptop&#34;,
            &#34;mouse&#34;,
            &#34;remote&#34;,
            &#34;keyboard&#34;,
            &#34;cell phone&#34;,
            &#34;microwave&#34;,
            &#34;oven&#34;,
            &#34;toaster&#34;,
            &#34;sink&#34;,
            &#34;refrigerator&#34;,
            &#34;book&#34;,
            &#34;clock&#34;,
            &#34;vase&#34;,
            &#34;scissors&#34;,
            &#34;teddy bear&#34;,
            &#34;hair drier&#34;,
            &#34;toothbrush&#34;,
        ],
        &#34;other&#34;: [&#34;aeroplane&#34;, &#34;train&#34;, &#34;boat&#34;],
    }

    def data_postprocessing(
        self, data: List[torch.FloatTensor], images: List[PIL.Image.Image]
    ) -&gt; List[List[str]]:
        &#34;&#34;&#34;
        Transforms output data from neural network to suitable data
        format for using with other components of this framework.

        Args:
            data: output data from neural network
            images: input images
        &#34;&#34;&#34;
        objects = super().data_postprocessing(data, images)
        new_output = []

        for image_objects in objects:
            new_objects = []
            for obj in image_objects:
                for key, values in list(self.db.items()):
                    if obj.class_name in values:
                        new_objects.append(key)  # We don&#39;t need bbox at this moment
            new_output.append(new_objects)

        return new_output</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="carvekit.ml.wrap.yolov4.SimplifiedYoloV4"><code class="flex name class">
<span>class <span class="ident">SimplifiedYoloV4</span></span>
<span>(</span><span>n_classes: int = 80, device='cpu', classes: List[str] = None, input_image_size: Union[List[int], int] = 608, batch_size: int = 4, load_pretrained: bool = True, fp16: bool = False, model_path: Union[str, pathlib.Path] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>The YoloV4 COCO classifier, but classifies only 7 supercategories.</p>
<p>human - Scenes of people, such as portrait photographs
animals - Scenes with animals
objects - Scenes with normal objects
cars - Scenes with cars
other - Other scenes</p>
<p>Initialize the YoloV4 COCO.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_classes</code></strong></dt>
<dd>number of classes</dd>
<dt><strong><code>device</code></strong></dt>
<dd>processing device</dd>
<dt><strong><code>input_image_size</code></strong></dt>
<dd>input image size</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>the number of images that the neural network processes in one run</dd>
<dt><strong><code>fp16</code></strong></dt>
<dd>use fp16 precision</dd>
<dt><strong><code>model_path</code></strong></dt>
<dd>path to model weights</dd>
<dt><strong><code>load_pretrained</code></strong></dt>
<dd>load pretrained weights</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SimplifiedYoloV4(YoloV4_COCO):
    &#34;&#34;&#34;
    The YoloV4 COCO classifier, but classifies only 7 supercategories.

    human - Scenes of people, such as portrait photographs
    animals - Scenes with animals
    objects - Scenes with normal objects
    cars - Scenes with cars
    other - Other scenes
    &#34;&#34;&#34;

    db = {
        &#34;human&#34;: [&#34;person&#34;],
        &#34;animals&#34;: [
            &#34;bird&#34;,
            &#34;cat&#34;,
            &#34;dog&#34;,
            &#34;horse&#34;,
            &#34;sheep&#34;,
            &#34;cow&#34;,
            &#34;elephant&#34;,
            &#34;bear&#34;,
            &#34;zebra&#34;,
            &#34;giraffe&#34;,
        ],
        &#34;cars&#34;: [
            &#34;car&#34;,
            &#34;motorbike&#34;,
            &#34;bus&#34;,
            &#34;truck&#34;,
        ],
        &#34;objects&#34;: [
            &#34;bicycle&#34;,
            &#34;traffic light&#34;,
            &#34;fire hydrant&#34;,
            &#34;stop sign&#34;,
            &#34;parking meter&#34;,
            &#34;bench&#34;,
            &#34;backpack&#34;,
            &#34;umbrella&#34;,
            &#34;handbag&#34;,
            &#34;tie&#34;,
            &#34;suitcase&#34;,
            &#34;frisbee&#34;,
            &#34;skis&#34;,
            &#34;snowboard&#34;,
            &#34;sports ball&#34;,
            &#34;kite&#34;,
            &#34;baseball bat&#34;,
            &#34;baseball glove&#34;,
            &#34;skateboard&#34;,
            &#34;surfboard&#34;,
            &#34;tennis racket&#34;,
            &#34;bottle&#34;,
            &#34;wine glass&#34;,
            &#34;cup&#34;,
            &#34;fork&#34;,
            &#34;knife&#34;,
            &#34;spoon&#34;,
            &#34;bowl&#34;,
            &#34;banana&#34;,
            &#34;apple&#34;,
            &#34;sandwich&#34;,
            &#34;orange&#34;,
            &#34;broccoli&#34;,
            &#34;carrot&#34;,
            &#34;hot dog&#34;,
            &#34;pizza&#34;,
            &#34;donut&#34;,
            &#34;cake&#34;,
            &#34;chair&#34;,
            &#34;sofa&#34;,
            &#34;pottedplant&#34;,
            &#34;bed&#34;,
            &#34;diningtable&#34;,
            &#34;toilet&#34;,
            &#34;tvmonitor&#34;,
            &#34;laptop&#34;,
            &#34;mouse&#34;,
            &#34;remote&#34;,
            &#34;keyboard&#34;,
            &#34;cell phone&#34;,
            &#34;microwave&#34;,
            &#34;oven&#34;,
            &#34;toaster&#34;,
            &#34;sink&#34;,
            &#34;refrigerator&#34;,
            &#34;book&#34;,
            &#34;clock&#34;,
            &#34;vase&#34;,
            &#34;scissors&#34;,
            &#34;teddy bear&#34;,
            &#34;hair drier&#34;,
            &#34;toothbrush&#34;,
        ],
        &#34;other&#34;: [&#34;aeroplane&#34;, &#34;train&#34;, &#34;boat&#34;],
    }

    def data_postprocessing(
        self, data: List[torch.FloatTensor], images: List[PIL.Image.Image]
    ) -&gt; List[List[str]]:
        &#34;&#34;&#34;
        Transforms output data from neural network to suitable data
        format for using with other components of this framework.

        Args:
            data: output data from neural network
            images: input images
        &#34;&#34;&#34;
        objects = super().data_postprocessing(data, images)
        new_output = []

        for image_objects in objects:
            new_objects = []
            for obj in image_objects:
                for key, values in list(self.db.items()):
                    if obj.class_name in values:
                        new_objects.append(key)  # We don&#39;t need bbox at this moment
            new_output.append(new_objects)

        return new_output</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="carvekit.ml.wrap.yolov4.YoloV4_COCO" href="#carvekit.ml.wrap.yolov4.YoloV4_COCO">YoloV4_COCO</a></li>
<li><a title="carvekit.ml.arch.yolov4.models.Yolov4" href="../arch/yolov4/models.html#carvekit.ml.arch.yolov4.models.Yolov4">Yolov4</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="carvekit.ml.wrap.yolov4.SimplifiedYoloV4.db"><code class="name">var <span class="ident">db</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="carvekit.ml.wrap.yolov4.SimplifiedYoloV4.data_postprocessing"><code class="name flex">
<span>def <span class="ident">data_postprocessing</span></span>(<span>self, data: List[torch.FloatTensor], images: List[PIL.Image.Image]) ‑> List[List[str]]</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms output data from neural network to suitable data
format for using with other components of this framework.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>output data from neural network</dd>
<dt><strong><code>images</code></strong></dt>
<dd>input images</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data_postprocessing(
    self, data: List[torch.FloatTensor], images: List[PIL.Image.Image]
) -&gt; List[List[str]]:
    &#34;&#34;&#34;
    Transforms output data from neural network to suitable data
    format for using with other components of this framework.

    Args:
        data: output data from neural network
        images: input images
    &#34;&#34;&#34;
    objects = super().data_postprocessing(data, images)
    new_output = []

    for image_objects in objects:
        new_objects = []
        for obj in image_objects:
            for key, values in list(self.db.items()):
                if obj.class_name in values:
                    new_objects.append(key)  # We don&#39;t need bbox at this moment
        new_output.append(new_objects)

    return new_output</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="carvekit.ml.wrap.yolov4.YoloV4_COCO" href="#carvekit.ml.wrap.yolov4.YoloV4_COCO">YoloV4_COCO</a></b></code>:
<ul class="hlist">
<li><code><a title="carvekit.ml.wrap.yolov4.YoloV4_COCO.data_preprocessing" href="#carvekit.ml.wrap.yolov4.YoloV4_COCO.data_preprocessing">data_preprocessing</a></code></li>
<li><code><a title="carvekit.ml.wrap.yolov4.YoloV4_COCO.forward" href="../arch/yolov4/models.html#carvekit.ml.arch.yolov4.models.Yolov4.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="carvekit.ml.wrap.yolov4.YoloV4_COCO"><code class="flex name class">
<span>class <span class="ident">YoloV4_COCO</span></span>
<span>(</span><span>n_classes: int = 80, device='cpu', classes: List[str] = None, input_image_size: Union[List[int], int] = 608, batch_size: int = 4, load_pretrained: bool = True, fp16: bool = False, model_path: Union[str, pathlib.Path] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>YoloV4 COCO model wrapper</p>
<p>Initialize the YoloV4 COCO.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_classes</code></strong></dt>
<dd>number of classes</dd>
<dt><strong><code>device</code></strong></dt>
<dd>processing device</dd>
<dt><strong><code>input_image_size</code></strong></dt>
<dd>input image size</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>the number of images that the neural network processes in one run</dd>
<dt><strong><code>fp16</code></strong></dt>
<dd>use fp16 precision</dd>
<dt><strong><code>model_path</code></strong></dt>
<dd>path to model weights</dd>
<dt><strong><code>load_pretrained</code></strong></dt>
<dd>load pretrained weights</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class YoloV4_COCO(Yolov4):
    &#34;&#34;&#34;YoloV4 COCO model wrapper&#34;&#34;&#34;

    def __init__(
        self,
        n_classes: int = 80,
        device=&#34;cpu&#34;,
        classes: List[str] = None,
        input_image_size: Union[List[int], int] = 608,
        batch_size: int = 4,
        load_pretrained: bool = True,
        fp16: bool = False,
        model_path: Union[str, pathlib.Path] = None,
    ):
        &#34;&#34;&#34;
        Initialize the YoloV4 COCO.

        Args:
            n_classes: number of classes
            device: processing device
            input_image_size: input image size
            batch_size: the number of images that the neural network processes in one run
            fp16: use fp16 precision
            model_path: path to model weights
            load_pretrained: load pretrained weights
        &#34;&#34;&#34;
        if model_path is None:
            model_path = yolov4_coco_pretrained()
        self.fp16 = fp16
        self.device = device
        self.batch_size = batch_size
        if isinstance(input_image_size, list):
            self.input_image_size = input_image_size[:2]
        else:
            self.input_image_size = (input_image_size, input_image_size)

        if load_pretrained:
            state_dict = torch.load(model_path, map_location=&#34;cpu&#34;)
            self.classes = state_dict[&#34;classes&#34;]
            super().__init__(n_classes=len(state_dict[&#34;classes&#34;]), inference=True)
            self.load_state_dict(state_dict[&#34;state&#34;])
        else:
            self.classes = classes
            super().__init__(n_classes=n_classes, inference=True)

        self.to(device)
        self.eval()

    def data_preprocessing(self, data: PIL.Image.Image) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;
        Transform input image to suitable data format for neural network

        Args:
            data: input image

        Returns:
            input for neural network

        &#34;&#34;&#34;
        image = data.resize(self.input_image_size)
        # noinspection PyTypeChecker
        image = np.array(image).astype(np.float32)
        image = image.transpose((2, 0, 1))
        image = image / 255.0
        image = torch.from_numpy(image).float()
        return torch.unsqueeze(image, 0).type(torch.FloatTensor)

    def data_postprocessing(
        self, data: List[torch.FloatTensor], images: List[PIL.Image.Image]
    ) -&gt; List[Object]:
        &#34;&#34;&#34;
        Transforms output data from neural network to suitable data
        format for using with other components of this framework.

        Args:
            data: output data from neural network
            images: input images


        Returns:
            list of objects for each image

        &#34;&#34;&#34;
        output = post_processing(0.4, 0.6, data)
        images_objects = []
        for image_idx, image_objects in enumerate(output):
            image_size = images[image_idx].size
            objects = []
            for obj in image_objects:
                objects.append(
                    Object(
                        class_name=self.classes[obj[6]],
                        confidence=obj[5],
                        x1=int(obj[0] * image_size[0]),
                        y1=int(obj[1] * image_size[1]),
                        x2=int(obj[2] * image_size[0]),
                        y2=int(obj[3] * image_size[1]),
                    )
                )
            images_objects.append(objects)

        return images_objects

    def __call__(
        self, images: List[Union[str, pathlib.Path, PIL.Image.Image]]
    ) -&gt; List[List[Object]]:
        &#34;&#34;&#34;
        Passes input images though neural network

        Args:
            images: input images

        Returns:
            list of objects for each image

        &#34;&#34;&#34;
        collect_masks = []
        autocast, dtype = get_precision_autocast(device=self.device, fp16=self.fp16)
        with autocast:
            cast_network(self, dtype)
            for image_batch in batch_generator(images, self.batch_size):
                converted_images = thread_pool_processing(
                    lambda x: convert_image(load_image(x)), image_batch
                )
                batches = torch.vstack(
                    thread_pool_processing(self.data_preprocessing, converted_images)
                )
                with torch.no_grad():
                    batches = Variable(batches).to(self.device)
                    out = super().__call__(batches)
                    out_cpu = [out_i.cpu() for out_i in out]
                    del batches, out
                out = self.data_postprocessing(out_cpu, converted_images)
                collect_masks += out

        return collect_masks</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="carvekit.ml.arch.yolov4.models.Yolov4" href="../arch/yolov4/models.html#carvekit.ml.arch.yolov4.models.Yolov4">Yolov4</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="carvekit.ml.wrap.yolov4.SimplifiedYoloV4" href="#carvekit.ml.wrap.yolov4.SimplifiedYoloV4">SimplifiedYoloV4</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="carvekit.ml.wrap.yolov4.YoloV4_COCO.data_postprocessing"><code class="name flex">
<span>def <span class="ident">data_postprocessing</span></span>(<span>self, data: List[torch.FloatTensor], images: List[PIL.Image.Image]) ‑> List[carvekit.ml.wrap.yolov4.Object]</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms output data from neural network to suitable data
format for using with other components of this framework.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>output data from neural network</dd>
<dt><strong><code>images</code></strong></dt>
<dd>input images</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>list of objects for each image</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data_postprocessing(
    self, data: List[torch.FloatTensor], images: List[PIL.Image.Image]
) -&gt; List[Object]:
    &#34;&#34;&#34;
    Transforms output data from neural network to suitable data
    format for using with other components of this framework.

    Args:
        data: output data from neural network
        images: input images


    Returns:
        list of objects for each image

    &#34;&#34;&#34;
    output = post_processing(0.4, 0.6, data)
    images_objects = []
    for image_idx, image_objects in enumerate(output):
        image_size = images[image_idx].size
        objects = []
        for obj in image_objects:
            objects.append(
                Object(
                    class_name=self.classes[obj[6]],
                    confidence=obj[5],
                    x1=int(obj[0] * image_size[0]),
                    y1=int(obj[1] * image_size[1]),
                    x2=int(obj[2] * image_size[0]),
                    y2=int(obj[3] * image_size[1]),
                )
            )
        images_objects.append(objects)

    return images_objects</code></pre>
</details>
</dd>
<dt id="carvekit.ml.wrap.yolov4.YoloV4_COCO.data_preprocessing"><code class="name flex">
<span>def <span class="ident">data_preprocessing</span></span>(<span>self, data: PIL.Image.Image) ‑> torch.FloatTensor</span>
</code></dt>
<dd>
<div class="desc"><p>Transform input image to suitable data format for neural network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>input image</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>input for neural network</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data_preprocessing(self, data: PIL.Image.Image) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;
    Transform input image to suitable data format for neural network

    Args:
        data: input image

    Returns:
        input for neural network

    &#34;&#34;&#34;
    image = data.resize(self.input_image_size)
    # noinspection PyTypeChecker
    image = np.array(image).astype(np.float32)
    image = image.transpose((2, 0, 1))
    image = image / 255.0
    image = torch.from_numpy(image).float()
    return torch.unsqueeze(image, 0).type(torch.FloatTensor)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="carvekit.ml.arch.yolov4.models.Yolov4" href="../arch/yolov4/models.html#carvekit.ml.arch.yolov4.models.Yolov4">Yolov4</a></b></code>:
<ul class="hlist">
<li><code><a title="carvekit.ml.arch.yolov4.models.Yolov4.forward" href="../arch/yolov4/models.html#carvekit.ml.arch.yolov4.models.Yolov4.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="carvekit.ml.wrap" href="index.html">carvekit.ml.wrap</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="carvekit.ml.wrap.yolov4.SimplifiedYoloV4" href="#carvekit.ml.wrap.yolov4.SimplifiedYoloV4">SimplifiedYoloV4</a></code></h4>
<ul class="">
<li><code><a title="carvekit.ml.wrap.yolov4.SimplifiedYoloV4.data_postprocessing" href="#carvekit.ml.wrap.yolov4.SimplifiedYoloV4.data_postprocessing">data_postprocessing</a></code></li>
<li><code><a title="carvekit.ml.wrap.yolov4.SimplifiedYoloV4.db" href="#carvekit.ml.wrap.yolov4.SimplifiedYoloV4.db">db</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="carvekit.ml.wrap.yolov4.YoloV4_COCO" href="#carvekit.ml.wrap.yolov4.YoloV4_COCO">YoloV4_COCO</a></code></h4>
<ul class="">
<li><code><a title="carvekit.ml.wrap.yolov4.YoloV4_COCO.data_postprocessing" href="#carvekit.ml.wrap.yolov4.YoloV4_COCO.data_postprocessing">data_postprocessing</a></code></li>
<li><code><a title="carvekit.ml.wrap.yolov4.YoloV4_COCO.data_preprocessing" href="#carvekit.ml.wrap.yolov4.YoloV4_COCO.data_preprocessing">data_preprocessing</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>