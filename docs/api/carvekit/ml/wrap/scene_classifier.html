<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>carvekit.ml.wrap.scene_classifier API documentation</title>
<meta name="description" content="Source url: https://github.com/OPHoperHPO/image-background-remove-tool
Author: Nikita Selin (OPHoperHPO)[https://github.com/OPHoperHPO].
License: …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>carvekit.ml.wrap.scene_classifier</code></h1>
</header>
<section id="section-intro">
<p>Source url: <a href="https://github.com/OPHoperHPO/image-background-remove-tool">https://github.com/OPHoperHPO/image-background-remove-tool</a>
Author: Nikita Selin (OPHoperHPO)[<a href="https://github.com/OPHoperHPO].">https://github.com/OPHoperHPO].</a>
License: Apache License 2.0</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Source url: https://github.com/OPHoperHPO/image-background-remove-tool
Author: Nikita Selin (OPHoperHPO)[https://github.com/OPHoperHPO].
License: Apache License 2.0
&#34;&#34;&#34;
import pathlib

import PIL.Image
import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
from typing import List, Union, Tuple
from torch.autograd import Variable

from carvekit.ml.files.models_loc import scene_classifier_pretrained
from carvekit.utils.image_utils import load_image, convert_image
from carvekit.utils.models_utils import get_precision_autocast, cast_network
from carvekit.utils.pool_utils import thread_pool_processing, batch_generator

__all__ = [&#34;SceneClassifier&#34;]


class SceneClassifier:
    &#34;&#34;&#34;
    SceneClassifier model interface

    Description:
        Performs a primary analysis of the image in order to select the necessary method for removing the background.
        The choice is made by classifying the scene type.

        The output can be the following types:
        - hard
        - soft
        - digital

    &#34;&#34;&#34;

    def __init__(
        self,
        topk: int = 1,
        device=&#34;cpu&#34;,
        batch_size: int = 4,
        fp16: bool = False,
        model_path: Union[str, pathlib.Path] = None,
    ):
        &#34;&#34;&#34;
        Initialize the Scene Classifier.

        Args:
            topk: number of top classes to return
            device: processing device
            batch_size: the number of images that the neural network processes in one run
            fp16: use fp16 precision

        &#34;&#34;&#34;
        if model_path is None:
            model_path = scene_classifier_pretrained()
        self.topk = topk
        self.fp16 = fp16
        self.device = device
        self.batch_size = batch_size

        self.transform = transforms.Compose(
            [
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
                ),
            ]
        )
        state_dict = torch.load(model_path, map_location=device)
        self.model = state_dict[&#34;model&#34;]
        self.class_to_idx = state_dict[&#34;class_to_idx&#34;]
        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}
        self.model.to(device)
        self.model.eval()

    def data_preprocessing(self, data: PIL.Image.Image) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;
        Transform input image to suitable data format for neural network

        Args:
            data: input image

        Returns:
            input for neural network

        &#34;&#34;&#34;

        return torch.unsqueeze(self.transform(data), 0).type(torch.FloatTensor)

    def data_postprocessing(self, data: torch.Tensor) -&gt; Tuple[List[str], List[float]]:
        &#34;&#34;&#34;
        Transforms output data from neural network to suitable data
        format for using with other components of this framework.

        Args:
            data: output data from neural network

        Returns:
            Top-k class of scene type, probability of these classes

        &#34;&#34;&#34;
        ps = F.softmax(data.float(), dim=0)
        topk = ps.cpu().topk(self.topk)

        probs, classes = (e.data.numpy().squeeze().tolist() for e in topk)
        if isinstance(classes, int):
            classes = [classes]
            probs = [probs]
        return list(map(lambda x: self.idx_to_class[x], classes)), probs

    def __call__(
        self, images: List[Union[str, pathlib.Path, PIL.Image.Image]]
    ) -&gt; Tuple[List[str], List[float]]:
        &#34;&#34;&#34;
        Passes input images though neural network and returns class predictions.

        Args:
            images: input images

        Returns:
            Top-k class of scene type, probability of these classes for every passed image

        &#34;&#34;&#34;
        collect_masks = []
        autocast, dtype = get_precision_autocast(device=self.device, fp16=self.fp16)
        with autocast:
            cast_network(self.model, dtype)
            for image_batch in batch_generator(images, self.batch_size):
                converted_images = thread_pool_processing(
                    lambda x: convert_image(load_image(x)), image_batch
                )
                batches = torch.vstack(
                    thread_pool_processing(self.data_preprocessing, converted_images)
                )
                with torch.no_grad():
                    batches = Variable(batches).to(self.device)
                    masks = self.model.forward(batches)
                    masks_cpu = masks.cpu()
                    del batches, masks
                masks = thread_pool_processing(
                    lambda x: self.data_postprocessing(masks_cpu[x]),
                    range(len(converted_images)),
                )
                collect_masks += masks

        return collect_masks</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="carvekit.ml.wrap.scene_classifier.SceneClassifier"><code class="flex name class">
<span>class <span class="ident">SceneClassifier</span></span>
<span>(</span><span>topk: int = 1, device='cpu', batch_size: int = 4, fp16: bool = False, model_path: Union[str, pathlib.Path] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>SceneClassifier model interface</p>
<h2 id="description">Description</h2>
<p>Performs a primary analysis of the image in order to select the necessary method for removing the background.
The choice is made by classifying the scene type.</p>
<p>The output can be the following types:
- hard
- soft
- digital</p>
<p>Initialize the Scene Classifier.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>topk</code></strong></dt>
<dd>number of top classes to return</dd>
<dt><strong><code>device</code></strong></dt>
<dd>processing device</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>the number of images that the neural network processes in one run</dd>
<dt><strong><code>fp16</code></strong></dt>
<dd>use fp16 precision</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SceneClassifier:
    &#34;&#34;&#34;
    SceneClassifier model interface

    Description:
        Performs a primary analysis of the image in order to select the necessary method for removing the background.
        The choice is made by classifying the scene type.

        The output can be the following types:
        - hard
        - soft
        - digital

    &#34;&#34;&#34;

    def __init__(
        self,
        topk: int = 1,
        device=&#34;cpu&#34;,
        batch_size: int = 4,
        fp16: bool = False,
        model_path: Union[str, pathlib.Path] = None,
    ):
        &#34;&#34;&#34;
        Initialize the Scene Classifier.

        Args:
            topk: number of top classes to return
            device: processing device
            batch_size: the number of images that the neural network processes in one run
            fp16: use fp16 precision

        &#34;&#34;&#34;
        if model_path is None:
            model_path = scene_classifier_pretrained()
        self.topk = topk
        self.fp16 = fp16
        self.device = device
        self.batch_size = batch_size

        self.transform = transforms.Compose(
            [
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
                ),
            ]
        )
        state_dict = torch.load(model_path, map_location=device)
        self.model = state_dict[&#34;model&#34;]
        self.class_to_idx = state_dict[&#34;class_to_idx&#34;]
        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}
        self.model.to(device)
        self.model.eval()

    def data_preprocessing(self, data: PIL.Image.Image) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;
        Transform input image to suitable data format for neural network

        Args:
            data: input image

        Returns:
            input for neural network

        &#34;&#34;&#34;

        return torch.unsqueeze(self.transform(data), 0).type(torch.FloatTensor)

    def data_postprocessing(self, data: torch.Tensor) -&gt; Tuple[List[str], List[float]]:
        &#34;&#34;&#34;
        Transforms output data from neural network to suitable data
        format for using with other components of this framework.

        Args:
            data: output data from neural network

        Returns:
            Top-k class of scene type, probability of these classes

        &#34;&#34;&#34;
        ps = F.softmax(data.float(), dim=0)
        topk = ps.cpu().topk(self.topk)

        probs, classes = (e.data.numpy().squeeze().tolist() for e in topk)
        if isinstance(classes, int):
            classes = [classes]
            probs = [probs]
        return list(map(lambda x: self.idx_to_class[x], classes)), probs

    def __call__(
        self, images: List[Union[str, pathlib.Path, PIL.Image.Image]]
    ) -&gt; Tuple[List[str], List[float]]:
        &#34;&#34;&#34;
        Passes input images though neural network and returns class predictions.

        Args:
            images: input images

        Returns:
            Top-k class of scene type, probability of these classes for every passed image

        &#34;&#34;&#34;
        collect_masks = []
        autocast, dtype = get_precision_autocast(device=self.device, fp16=self.fp16)
        with autocast:
            cast_network(self.model, dtype)
            for image_batch in batch_generator(images, self.batch_size):
                converted_images = thread_pool_processing(
                    lambda x: convert_image(load_image(x)), image_batch
                )
                batches = torch.vstack(
                    thread_pool_processing(self.data_preprocessing, converted_images)
                )
                with torch.no_grad():
                    batches = Variable(batches).to(self.device)
                    masks = self.model.forward(batches)
                    masks_cpu = masks.cpu()
                    del batches, masks
                masks = thread_pool_processing(
                    lambda x: self.data_postprocessing(masks_cpu[x]),
                    range(len(converted_images)),
                )
                collect_masks += masks

        return collect_masks</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="carvekit.ml.wrap.scene_classifier.SceneClassifier.data_postprocessing"><code class="name flex">
<span>def <span class="ident">data_postprocessing</span></span>(<span>self, data: torch.Tensor) ‑> Tuple[List[str], List[float]]</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms output data from neural network to suitable data
format for using with other components of this framework.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>output data from neural network</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Top-k class of scene type, probability of these classes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data_postprocessing(self, data: torch.Tensor) -&gt; Tuple[List[str], List[float]]:
    &#34;&#34;&#34;
    Transforms output data from neural network to suitable data
    format for using with other components of this framework.

    Args:
        data: output data from neural network

    Returns:
        Top-k class of scene type, probability of these classes

    &#34;&#34;&#34;
    ps = F.softmax(data.float(), dim=0)
    topk = ps.cpu().topk(self.topk)

    probs, classes = (e.data.numpy().squeeze().tolist() for e in topk)
    if isinstance(classes, int):
        classes = [classes]
        probs = [probs]
    return list(map(lambda x: self.idx_to_class[x], classes)), probs</code></pre>
</details>
</dd>
<dt id="carvekit.ml.wrap.scene_classifier.SceneClassifier.data_preprocessing"><code class="name flex">
<span>def <span class="ident">data_preprocessing</span></span>(<span>self, data: PIL.Image.Image) ‑> torch.FloatTensor</span>
</code></dt>
<dd>
<div class="desc"><p>Transform input image to suitable data format for neural network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>input image</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>input for neural network</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data_preprocessing(self, data: PIL.Image.Image) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;
    Transform input image to suitable data format for neural network

    Args:
        data: input image

    Returns:
        input for neural network

    &#34;&#34;&#34;

    return torch.unsqueeze(self.transform(data), 0).type(torch.FloatTensor)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="carvekit.ml.wrap" href="index.html">carvekit.ml.wrap</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="carvekit.ml.wrap.scene_classifier.SceneClassifier" href="#carvekit.ml.wrap.scene_classifier.SceneClassifier">SceneClassifier</a></code></h4>
<ul class="">
<li><code><a title="carvekit.ml.wrap.scene_classifier.SceneClassifier.data_postprocessing" href="#carvekit.ml.wrap.scene_classifier.SceneClassifier.data_postprocessing">data_postprocessing</a></code></li>
<li><code><a title="carvekit.ml.wrap.scene_classifier.SceneClassifier.data_preprocessing" href="#carvekit.ml.wrap.scene_classifier.SceneClassifier.data_preprocessing">data_preprocessing</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>